# ğŸ§  Advanced Structured JSON Extractor v2.0: Unstructured Text â†’ Structured JSON

> Enterprise-grade document processing system that converts complex unstructured documents into valid structured JSON using state-of-the-art LLMs with intelligent chunking, schema partitioning, and multi-pass extraction.

---

## ğŸ”— App Access

**Streamlit App**: [https://json-extract-gen.streamlit.app/](https://json-extract-gen.streamlit.app/)

---

## ğŸš€ How to Use the App

### Quick Start (Simple Pipeline)
1. Visit the app link above.
2. Upload a valid JSON schema.
3. Upload a text file (`.txt` or `.pdf`) with unstructured content.
4. Choose your model from the dropdown:
   - ğŸ§  **GPT 4.1** (128K context) â€” Balanced performance & cost
   - ğŸš€ **O4 Mini** (128K context) â€” Cost-effective for large documents
   - ğŸ¯ **Claude Sonnet 4.0** (200K context) â€” Best for complex schemas
   - ğŸ’¬ **Chat GPT O3** (128K context) â€” Advanced reasoning
5. Review file analysis (tokens, nesting level, complexity score).
6. Select **Simple Pipeline** for documents < 10K tokens.
7. View extracted JSON with validation status.
8. Download the JSON output and session logs.

### Advanced Usage (Extensive Pipeline)
For large documents (>10K tokens) or complex schemas:

1. After file upload and analysis, select **Extensive Pipeline**.
2. **Step 1 - Text Analysis**:
   - Review semantic structure analysis
   - Approve/regenerate/skip intelligent chunking strategy
3. **Step 2 - Schema Analysis**:
   - Review schema complexity metrics
   - Approve/regenerate/skip schema partitioning
4. **Step 3 - Multi-Pass Extraction**:
   - Choose 2-5 extraction passes
   - System performs deep extraction with context awareness
5. **Step 4 - Intelligent Merging**:
   - Automatic merging of all extractions
   - Schema compliance enforcement
6. Download validated JSON and detailed logs.

---

## ğŸ—ï¸ Architecture Overview

### Two Processing Pipelines

#### âš¡ Simple Pipeline
- Direct extraction for small documents
- Single-pass processing with 3 retry attempts
- Best for: Documents < 10K tokens, simple schemas

#### ğŸš€ Extensive Pipeline
- **Intelligent Text Chunking**: Semantic-aware document splitting
- **Schema Partitioning**: Break complex schemas into manageable parts
- **Multi-Pass Extraction**: 2-5 passes with increasing depth
- **Context-Aware Processing**: Embeddings maintain document coherence
- **Intelligent Merging**: Programmatic conflict resolution
- Best for: Large documents, complex nested schemas, high accuracy requirements

---

## ğŸ§± System Architecture

### High-Level Components
![alt text](image.png)

### Detailed Components
![alt text](image-1.png)


### Core Modules

| Module | Purpose |
|--------|---------|
| **Text Analyzer** | Semantic structure discovery, intelligent chunking |
| **Schema Analyzer** | Complexity calculation, partitioning strategy |
| **Extraction Engine** | Multi-pass extraction with validation |
| **Merger Engine** | Intelligent merging with schema compliance |
| **Embedding Manager** | Context preservation across chunks |
| **Token Estimator** | Smart token management & truncation |
| **Session Manager** | State tracking & persistence |

---

## ğŸ§° Technologies Used

- **LLM APIs**: OpenAI GPT-4.1, O4 Mini, O3 + Anthropic Claude Sonnet 4.0
- **Frontend**: Streamlit with real-time progress tracking
- **Embeddings**: Sentence-Transformers (384-dim vectors)
- **Validation**: JSON Schema Draft 7 + jsonschema
- **Token Management**: tiktoken with model-specific limits
- **Prompting**: Adaptive multi-pass with few-shot learning
- **Logging**: Thread-safe, structured session logs
- **Processing**: Concurrent chunk/partition processing

---

## ğŸ“ Project Structure

```text
JSON-EXTRACTOR-V2/
â”œâ”€â”€ core/                         # Core processing modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extraction_engine.py     # Multi-pass extraction logic
â”‚   â”œâ”€â”€ merger_engine.py         # Intelligent merging system
â”‚   â”œâ”€â”€ text_analyzer.py         # Semantic text analysis
â”‚   â”œâ”€â”€ schema_analyzer.py       # Schema complexity analysis
â”‚   â”œâ”€â”€ embedding_manager.py     # Vector embeddings & similarity
â”‚   â”œâ”€â”€ json_extractor.py        # JSON extraction & validation
â”‚   â”œâ”€â”€ llm_interface.py         # Multi-model LLM interface
â”‚   â”œâ”€â”€ logger_service.py        # Comprehensive logging
â”‚   â”œâ”€â”€ prompt_engine.py         # Adaptive prompt generation
â”‚   â”œâ”€â”€ session_manager.py       # Session state management
â”‚   â””â”€â”€ token_estimator.py       # Advanced token management
â”‚
â”œâ”€â”€ logs/                         # Session logs & outputs
â”‚   â””â”€â”€ [session-id]/
â”‚       â”œâ”€â”€ metadata.json
â”‚       â”œâ”€â”€ session.log
â”‚       â””â”€â”€ output.json
â”‚
â”œâ”€â”€ embeddings_cache/             # Cached embeddings
â”‚
â”œâ”€â”€ Sample Inputs/                # Test cases & examples
â”‚
â”œâ”€â”€ .env                          # API keys (OPENAI_API_KEY, ANTHROPIC_API_KEY)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ app.py                        # Streamlit application
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”§ Local Setup

```bash
# Clone repository
git clone https://github.com/[your-repo]/json-extractor-v2.git
cd json-extractor-v2

# Install dependencies
pip install -r requirements.txt

# Configure API keys
echo "OPENAI_API_KEY=sk-..." > .env
echo "ANTHROPIC_API_KEY=sk-ant-..." >> .env

# Run Streamlit app
streamlit run app.py
```

---

## ğŸ“Š Key Features

### ğŸ¯ Intelligent Processing
- **Semantic Analysis**: Understands document structure and information density
- **Adaptive Chunking**: Respects natural boundaries (paragraphs, sections)
- **Schema Partitioning**: Handles schemas with 100+ fields efficiently
- **Multi-Pass Extraction**: Progressive refinement for accuracy

### ğŸ”’ Reliability
- **Schema Validation**: Strict Draft 7 compliance
- **Type Coercion**: Automatic type fixing
- **Error Recovery**: Programmatic and LLM-based fixing
- **Token Safety**: Never truncates critical schema information

### ğŸ“ˆ Performance
- **Optimized Mode**: Skip merging for single chunk/partition
- **Parallel Processing**: Concurrent chunk-partition extraction
- **Smart Caching**: Embedding reuse across sessions
- **Token Efficiency**: Intelligent prompt optimization

### ğŸ“ Observability
- **Real-Time Progress**: Live updates during processing
- **Detailed Metrics**: Tokens, complexity, field counts
- **Comprehensive Logs**: Every step logged with timestamps
- **Export Options**: JSON, TXT, CSV log formats

---

## ğŸ’¡ Usage Tips

1. **Model Selection**:
   - Use Claude Sonnet 4.0 for complex nested schemas
   - GPT-4.1 for balanced performance
   - O4 Mini for cost-effective large document processing

2. **Pipeline Selection**:
   - Simple: Documents < 10K tokens, straightforward schemas
   - Extensive: Large documents, complex schemas, critical accuracy

3. **Optimization**:
   - Approve chunking for documents > 5K tokens
   - Partition schemas with > 50 fields
   - Use 3-pass extraction for optimal accuracy

4. **Troubleshooting**:
   - Check logs for token limit issues
   - Ensure schema is Draft 7 compliant
   - Verify API keys are correctly configured

---

## ğŸ“š Documentation

ğŸ§¾ View detailed documentation [click here](https://docs.google.com/document/d/e/2PACX-1vR-vEbZA1sojVCf9MwVLlU0KaYF_In8wGCkgnSmUXlcDM7GSDSKnjx_3lQi99mMnx8RTdrg8O6LKEW3/pub).

---

## ğŸš€ What's New in v2.0

- âœ¨ **Extensive Pipeline**: Advanced processing for complex documents
- ğŸ§© **Schema Partitioning**: Handle schemas with 500+ fields
- ğŸ”„ **Multi-Pass Extraction**: 5-pass deep extraction system
- ğŸ§  **Embedding Integration**: Semantic context preservation
- ğŸ¤– **Claude Support**: Added Anthropic Claude Sonnet 4.0
- ğŸ“Š **Advanced Analytics**: Complexity scoring & metrics
- ğŸ”§ **Smart Merging**: Programmatic conflict resolution
- ğŸ“ˆ **100K+ Token Support**: Process entire books/reports

---

## ğŸ‘¤ Author & Support

Developed by **Avijeet Palit**  
ğŸ“« Contact: [avijeet.palit07@gmail.com]  
ğŸ› Issues: [GitHub Issues](https://github.com/[your-repo]/issues)  
ğŸ’¬ Discussions: [GitHub Discussions](https://github.com/[your-repo]/discussions)